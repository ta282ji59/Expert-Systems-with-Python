{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af75dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog\n",
    "import tkinter.messagebox\n",
    "import math\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96844e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_file_path(message):\n",
    "    # tkinterというGUIを用いて読み込みたいファイルを選択してもらう\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    fTyp = [(\"\",\"txt\")]\n",
    "    iDir = os.path.abspath(os.path.dirname('__file__'))\n",
    "    tk.messagebox.showinfo('Text_Analysis_Tool',message)\n",
    "    datafile = tk.filedialog.askopenfilename(filetypes = fTyp,initialdir = iDir)\n",
    "    return(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d18d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        list = f.readlines()\n",
    "\n",
    "    new_list = []\n",
    "\n",
    "    for i in list:\n",
    "        word = i.split()\n",
    "        new_list.append(word)\n",
    "    return(new_list)\n",
    "\n",
    "ignore_list = [\"\",\" \", \"  \", \"   \", \"    \"] #list of items we want to ignore in our frequency calculations\n",
    "\n",
    "def corpus_frequency(corpus_list, ignore = ignore_list, calc = 'freq', normed = False): #options for calc are 'freq' or 'range'\n",
    "        freq_dict = {} #empty dictionary\n",
    "\n",
    "        for tokenized in corpus_list: #iterate through the tokenized texts\n",
    "            if calc == 'range': #if range was selected:\n",
    "                tokenized = list(set(tokenized)) #this creates a list of types (unique words)\n",
    "\n",
    "            for token in tokenized: #iterate through each word in the texts\n",
    "                if token in ignore_list: #if token is in ignore list\n",
    "                    continue #move on to next word\n",
    "                if token not in freq_dict: #if the token isn't already in the dictionary:\n",
    "                    freq_dict[token] = 1 #set the token as the key and the value as 1\n",
    "                else: #if it is in the dictionary\n",
    "                    freq_dict[token] += 1 #add one to the count\n",
    "\n",
    "        ### Normalization:\n",
    "        if normed == True and calc == 'freq':\n",
    "            corp_size = sum(freq_dict.values()) #this sums all of the values in the dictionary\n",
    "            for x in freq_dict:\n",
    "                freq_dict[x] = freq_dict[x]/corp_size * 1000000 #norm per million words\n",
    "        elif normed == True and calc == \"range\":\n",
    "            corp_size = len(corpus_list) #number of documents in corpus\n",
    "            for x in freq_dict:\n",
    "                freq_dict[x] = freq_dict[x]/corp_size * 100 #create percentage (norm by 100)\n",
    "\n",
    "        return(freq_dict)\n",
    "\n",
    "# Assuming you already have a 'collocation_results' dictionary containing collocation scores\n",
    "def high_val(stat_dict,hits = 20,hsort = True,output = False,filename = None, sep = \"\\t\"):\n",
    "        #first, create sorted list. Presumes that operator has been imported\n",
    "        sorted_list = sorted(stat_dict.items(),key=operator.itemgetter(1),reverse = hsort)[:hits]\n",
    "\n",
    "        if output == False and filename == None: #if we aren't writing a file or returning a list\n",
    "            for x in sorted_list: #iterate through the output\n",
    "                print(x[0] + \"\\t\" + str(x[1])) #print the sorted list in a nice format\n",
    "\n",
    "        elif filename is not None: #if a filename was provided\n",
    "            outf = open(filename,\"w\") #create a blank file in the working directory using the filename\n",
    "            for x in sorted_list: #iterate through list\n",
    "                outf.write(x[0] + sep + str(x[1])+\"\\n\") #write each line to a file using the separator\n",
    "            outf.flush() #flush the file buffer\n",
    "            outf.close() #close the file\n",
    "\n",
    "        if output == True: #if output is true\n",
    "            return(sorted_list) #return the sorted list\n",
    "\n",
    "def read_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().split()\n",
    "        # 単語を小文字に統一して返す\n",
    "        return [word.lower() for word in words]\n",
    "    \n",
    "def find_common_words(file_paths):\n",
    "    # 各ファイルの単語リストを読み込む\n",
    "    word_lists = [read_words_from_file(file_path) for file_path in file_paths]\n",
    "\n",
    "    # 共通する単語を見つける\n",
    "    common_words = set(word_lists[0]).intersection(*word_lists[1:])\n",
    "\n",
    "    return common_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec841226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_similar_dataset(questioned_dataset, datasets):\n",
    "    # Calculate the keyword frequencies for the questioned dataset\n",
    "    questioned_freq_dict = corpus_frequency(questioned_dataset, ignore=ignore_list, calc='freq', normed=True)\n",
    "\n",
    "    # Calculate the common top 20 keywords between questioned dataset and each dataset\n",
    "    common_keywords_counts = {}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset_freq_dict = corpus_frequency(dataset, ignore=ignore_list, calc='freq', normed=True)\n",
    "        common_keywords = set(questioned_freq_dict.keys()) & set(dataset_freq_dict.keys())\n",
    "        common_keywords_counts[dataset_name] = len(common_keywords)\n",
    "\n",
    "    # Find the dataset with the least common top 20 keywords\n",
    "    least_similar_dataset = min(common_keywords_counts, key=common_keywords_counts.get)\n",
    "\n",
    "    return least_similar_dataset\n",
    "\n",
    "# # Example usage:\n",
    "# datasets = {\n",
    "#     \"dataset1\": corpus_list1,\n",
    "#     \"dataset2\": corpus_list2,\n",
    "# }\n",
    "\n",
    "# # questioned_dataset_name = \"questioned_dataset\"\n",
    "# questioned_dataset = corpus_list3 # ここに質問されたデータセットを指定してください\n",
    "\n",
    "# # Find the least similar dataset and suggest to exclude it\n",
    "# least_similar_dataset = find_least_similar_dataset(questioned_dataset, datasets)\n",
    "# print(\"The least similar dataset is:\", least_similar_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19418f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)  # Download the 'punkt' resource (if you haven't done it before)\n",
    "\n",
    "def pos_tag_english_text(text_list):\n",
    "    pos_tags_list = []\n",
    "    for text_tokens in text_list:\n",
    "        pos_tags = nltk.pos_tag(text_tokens)\n",
    "        pos_tags_list.append(pos_tags)\n",
    "    return pos_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486f0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_context(text, target_word, window_size):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Find the target word in the tokenized text\n",
    "    target_indices = []\n",
    "    for i, sentence in enumerate(words):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word.lower() == target_word.lower():\n",
    "                target_indices.append((i, j))\n",
    "\n",
    "    # Extract the context around the target word\n",
    "    contexts = []\n",
    "    for sentence_index, word_index in target_indices:\n",
    "        start = max(0, word_index - window_size)\n",
    "        end = min(len(words[sentence_index]), word_index + window_size + 1)\n",
    "        context = words[sentence_index][start:end]\n",
    "        contexts.append(\" \".join(context))\n",
    "\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebf33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_pattern(text, target_word):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Find the target word in the tokenized text\n",
    "    target_indices = [i for i, word in enumerate(words) if word.lower() == target_word.lower()]\n",
    "\n",
    "    # Extract the POS patterns following the target word\n",
    "    pos_patterns = []\n",
    "    for idx in target_indices:\n",
    "        if idx < len(words) - 1:  # Ensure there's a word following the target word\n",
    "            target_pos = pos_tag([words[idx]])[0][1]\n",
    "            next_pos = pos_tag([words[idx + 1]])[0][1]\n",
    "            pos_pattern = f\"{target_word} + {next_pos}\"\n",
    "            pos_patterns.append(pos_pattern)\n",
    "\n",
    "    return pos_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ac1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_pos_patterns(dataset):\n",
    "    # Join the sentences in the dataset into a single string\n",
    "    text = ' '.join([' '.join(sentence) for sentence in dataset])\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Perform POS tagging on the words\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Extract POS patterns for all words\n",
    "    pos_patterns = []\n",
    "    for word_idx in range(len(pos_tags) - 1):\n",
    "        target_word = pos_tags[word_idx][0]\n",
    "        target_pos = pos_tags[word_idx][1]\n",
    "        next_pos = pos_tags[word_idx + 1][1]\n",
    "        pos_pattern = f\"{target_word} + {next_pos}\"\n",
    "        pos_patterns.append(pos_pattern)\n",
    "\n",
    "    # Count identical POS patterns\n",
    "    pos_pattern_counts = {}\n",
    "    for pattern in pos_patterns:\n",
    "        if pattern in pos_pattern_counts:\n",
    "            pos_pattern_counts[pattern] += 1\n",
    "        else:\n",
    "            pos_pattern_counts[pattern] = 1\n",
    "\n",
    "    return pos_pattern_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2ce7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_pos_similar_dataset(questioned_dataset, datasets):\n",
    "    # Calculate POS pattern counts for the questioned dataset\n",
    "    questioned_pos_patterns = count_all_pos_patterns(questioned_dataset)\n",
    "\n",
    "    # Calculate POS pattern counts for each dataset\n",
    "    pos_pattern_counts_per_dataset = {}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        pos_pattern_counts_per_dataset[dataset_name] = count_all_pos_patterns(dataset)\n",
    "\n",
    "    # Calculate the difference in POS pattern counts between questioned dataset and each dataset\n",
    "    pattern_differences = {}\n",
    "    for dataset_name, pos_pattern_counts in pos_pattern_counts_per_dataset.items():\n",
    "        difference = 0\n",
    "        for pattern, count in questioned_pos_patterns.items():\n",
    "            if pattern in pos_pattern_counts:\n",
    "                difference += abs(count - pos_pattern_counts[pattern])\n",
    "            else:\n",
    "                difference += count\n",
    "        pattern_differences[dataset_name] = difference\n",
    "\n",
    "    # Find the dataset with the least similarity in POS patterns\n",
    "    least_similar_dataset = min(pattern_differences, key=pattern_differences.get)\n",
    "\n",
    "    return least_similar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c3004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 06:29:53.488 python[27225:519583] +[CATransaction synchronize] called within transaction\n",
      "2023-07-30 06:29:56.161 python[27225:519583] +[CATransaction synchronize] called within transaction\n",
      "2023-07-30 06:29:58.324 python[27225:519583] +[CATransaction synchronize] called within transaction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "is\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mコードを実行できません。セッションは破棄されました。カーネルを再起動してください。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "def create_count_window():\n",
    "    # 別のウィンドウを作成\n",
    "    new_window = tk.Toplevel(window)\n",
    "    new_window.title(\"count\")\n",
    "\n",
    "    # メッセージを表示するラベルを作成\n",
    "    label = tk.Label(new_window, text=\"dataset1\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text1 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text1.pack(padx=10, pady=5)\n",
    "    \n",
    "    label = tk.Label(new_window, text=\"dataset2\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text2 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text2.pack(padx=10, pady=5)\n",
    "    \n",
    "    label = tk.Label(new_window, text=\"questioned dataset\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text3 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text3.pack(padx=10, pady=5)\n",
    "    \n",
    "\n",
    "    high_list1 = high_val(freq_dict1, output = True)\n",
    "    high_list2 = high_val(freq_dict2, output = True)\n",
    "    high_list3 = high_val(freq_dict3, output = True)\n",
    "\n",
    "    pos_pattern_counts1 = count_all_pos_patterns(corpus_list1)\n",
    "    pos_pattern_counts2 = count_all_pos_patterns(corpus_list2)\n",
    "    pos_pattern_counts3 = count_all_pos_patterns(corpus_list3)\n",
    "    \n",
    "    def format_pos_patterns(pos_pattern_counts):\n",
    "        # テキストの整形\n",
    "        formatted_text = \"\"\n",
    "        for pattern, count in pos_pattern_counts.items():\n",
    "            formatted_text += f\"{pattern}: {count}\\n\"\n",
    "\n",
    "        return formatted_text\n",
    "    result_text1 = \"—————top 20 of frequency————— \\n\"\n",
    "    result_text1 += \"\\n\".join([f\"{index + 1}. {item[0]}\" for index, item in enumerate(high_list1)])\n",
    "    result_text1 += \"\\n\\n———————POS pattern———————\\n\"\n",
    "    result_text1 += format_pos_patterns(pos_pattern_counts1)\n",
    "    \n",
    "    result_text2 = \"—————top 20 of frequency————— \\n\"\n",
    "    result_text2 += \"\\n\".join([f\"{index + 1}. {item[0]}\" for index, item in enumerate(high_list2)])\n",
    "    result_text2 += \"\\n\\n----------POS pattern----------\\n\"\n",
    "    result_text2 += format_pos_patterns(pos_pattern_counts2)\n",
    "\n",
    "    result_text3 = \"—————top 20 of frequency————— \\n\"\n",
    "    result_text3 += \"\\n\".join([f\"{index + 1}. {item[0]}\" for index, item in enumerate(high_list3)])\n",
    "    result_text3 += \"\\n\\n----------POS pattern----------\\n\"\n",
    "    result_text3 += format_pos_patterns(pos_pattern_counts3)\n",
    "    \n",
    "    # 実行結果をウィンドウに表示\n",
    "    output_text1.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "    output_text1.insert(tk.END, result_text1)\n",
    "    output_text2.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "    output_text2.insert(tk.END, result_text2)\n",
    "    output_text3.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "    output_text3.insert(tk.END, result_text3)  \n",
    "\n",
    "    # leaset similar datasetメッセージを表示するラベルを作成\n",
    "    questioned_dataset = corpus_list3 # ここに質問されたデータセットを指定してください\n",
    "\n",
    "    # Find the least similar dataset and suggest to exclude it\n",
    "    least_similar_dataset = find_least_similar_dataset(questioned_dataset, datasets)\n",
    "\n",
    "    label = tk.Label(new_window, text=\"The least similar dataset is {} based on freqency\".format(least_similar_dataset))\n",
    "    label.pack(padx=20, pady=10)\n",
    "    \n",
    "def create_list_window():\n",
    "    # 別のウィンドウを作成\n",
    "    new_window = tk.Toplevel(window)\n",
    "    new_window.title(\"list\")\n",
    "\n",
    "    # メッセージを表示するラベルを作成\n",
    "    label = tk.Label(new_window, text=\"display the shared words/keywords of each dataset\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "\n",
    "    output_text = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text.pack(padx=10, pady=5)\n",
    "\n",
    "    common_keywords = find_common_words([file_path1, file_path2, file_path3])\n",
    "    shared_words_list = list(common_keywords)\n",
    "\n",
    "    # Prepare the formatted result_text as a single string\n",
    "    result_text = \"\\n\".join(f\"{index + 1}. {keyword}\" for index, keyword in enumerate(shared_words_list))\n",
    "\n",
    "\n",
    "    output_text.delete(1.0, tk.END)\n",
    "    output_text.insert(tk.END, result_text)\n",
    "    \n",
    "def create_pos_window():\n",
    "    # OKボタンをクリックしたときの処理\n",
    "    def ok_button_clicked():\n",
    "        target_word = entry_var.get()\n",
    "        print(target_word)\n",
    "        # エントリーウィジェットで入力されたテキストを取得\n",
    "        if target_word == 'all':\n",
    "            # 実行結果をウィンドウに表示\n",
    "            pos_tags1 = pos_tag_english_text(corpus_list1)\n",
    "            pos_tags2 = pos_tag_english_text(corpus_list2)\n",
    "            pos_tags3 = pos_tag_english_text(corpus_list3)\n",
    "\n",
    "            result_text1 = \"\\n\".join([f\"{item[0]} + {item[1]}\" for pos_tags_sentence in pos_tags1 for item in pos_tags_sentence])\n",
    "            result_text2 = \"\\n\".join([f\"{item[0]} + {item[1]}\" for pos_tags_sentence in pos_tags2 for item in pos_tags_sentence])\n",
    "            result_text3 = \"\\n\".join([f\"{item[0]} + {item[1]}\" for pos_tags_sentence in pos_tags3 for item in pos_tags_sentence])\n",
    "            \n",
    "            output_text1.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text1.insert(tk.END, result_text1)\n",
    "            output_text2.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text2.insert(tk.END, result_text2)\n",
    "            output_text3.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text3.insert(tk.END, result_text3)\n",
    "        else:\n",
    "            pos_patterns1 = find_pos_pattern(text1, target_word)\n",
    "            pos_patterns2 = find_pos_pattern(text2, target_word)\n",
    "            pos_patterns3 = find_pos_pattern(text3, target_word)\n",
    "            \n",
    "            contexts1 = find_word_context(text1, target_word, window_size=6)\n",
    "            contexts2 = find_word_context(text2, target_word, window_size=6)\n",
    "            contexts3 = find_word_context(text3, target_word, window_size=6)\n",
    "            \n",
    "            # pos_patternsを文字列に変換して改行して表示\n",
    "            result_text1 = \"\\n\".join(str(item) for item in pos_patterns1)\n",
    "            result_text1 += \"\\n\\n----------context----------\\n\"\n",
    "            result_text2 = \"\\n\".join(str(item) for item in pos_patterns2)\n",
    "            result_text2 += \"\\n\\n----------context----------\\n\"\n",
    "            result_text3 = \"\\n\".join(str(item) for item in pos_patterns3)\n",
    "            result_text3 += \"\\n\\n----------context----------\\n\"\n",
    "            \n",
    "            output_text1.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text1.insert(tk.END, result_text1)\n",
    "            for idx, context in enumerate(contexts1, start=1):\n",
    "                output_text1.insert(tk.END, f\"Context {idx}: {context}\\n\\n\")\n",
    "                \n",
    "            output_text2.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text2.insert(tk.END, result_text2)\n",
    "            for idx, context in enumerate(contexts2, start=1):\n",
    "                output_text2.insert(tk.END, f\"Context {idx}: {context}\\n\\n\")\n",
    "                \n",
    "            output_text3.delete(1.0, tk.END)  # 既存のテキストを削除\n",
    "            output_text3.insert(tk.END, result_text3)\n",
    "            for idx, context in enumerate(contexts3, start=1):\n",
    "                output_text3.insert(tk.END, f\"Context {idx}: {context}\\n\\n\")\n",
    "\n",
    "    # 別のウィンドウを作成\n",
    "    new_window = tk.Toplevel(window)\n",
    "    new_window.title(\"pos\")\n",
    "\n",
    "    # メッセージを表示するラベルを作成\n",
    "    label = tk.Label(new_window, text=\"Please enter your target word:\\n if input all, then you can show all pos.\")\n",
    "    label.pack(padx=20, pady=10)     \n",
    "    # テキスト入力用のエントリーウィジェットを作成\n",
    "    entry_var = tk.StringVar()  # 変数を作成\n",
    "    entry = tk.Entry(new_window, textvariable=entry_var)\n",
    "    entry.pack(padx=20, pady=10)\n",
    "\n",
    "    # OKボタンを作成\n",
    "    ok_button = tk.Button(new_window, text=\"OK\", command=ok_button_clicked)\n",
    "    ok_button.pack(pady=10)\n",
    "    \n",
    "    label = tk.Label(new_window, text=\"dataset1\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text1 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text1.pack(padx=10, pady=5)\n",
    "    \n",
    "    label = tk.Label(new_window, text=\"dataset2\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text2 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text2.pack(padx=10, pady=5)\n",
    "    \n",
    "    label = tk.Label(new_window, text=\"questioned dataset\")\n",
    "    label.pack(padx=20, pady=10)\n",
    "    output_text3 = tk.Text(new_window, wrap=tk.WORD, height=10, width=40)\n",
    "    output_text3.pack(padx=10, pady=5)\n",
    "\n",
    "    # leaset similar datasetメッセージを表示するラベルを作成\n",
    "    questioned_dataset = text3  # Replace with the questioned dataset\n",
    "    # Find the least similar dataset and suggest to exclude it\n",
    "    least_similar_dataset = find_least_pos_similar_dataset(questioned_dataset, datasets)\n",
    "\n",
    "    label = tk.Label(new_window, text=\"The least similar dataset is {} based on pos pattern\".format(least_similar_dataset))\n",
    "    label.pack(padx=20, pady=10)\n",
    "    \n",
    "\n",
    "# ウィンドウを作成\n",
    "window = tk.Tk()\n",
    "window.title(\"Text_Analysis_Tool\")\n",
    "window.geometry('300x300')     # 表示画面サイズ（幅x高さ)\n",
    "\n",
    "file_path3 = get_text_file_path('Select the .txt file that will be the \\'Questioned dataset\\'')\n",
    "file_path1 = get_text_file_path('Select the .txt file that will be the \\'First comparison target\\'')\n",
    "file_path2 = get_text_file_path('Select the .txt file that will be the \\'Second comparison target\\'')\n",
    "\n",
    "text1 = open(file_path1, encoding=\"UTF-8\").read()\n",
    "text2 = open(file_path2, encoding=\"UTF-8\").read()\n",
    "text3 = open(file_path3, encoding=\"UTF-8\").read()\n",
    "\n",
    "corpus_list1 = read_text_file(file_path1)\n",
    "corpus_list2 = read_text_file(file_path2)\n",
    "corpus_list3 = read_text_file(file_path3)\n",
    "\n",
    "freq_dict1 = corpus_frequency(corpus_list1, ignore=ignore_list, calc='freq', normed=True)\n",
    "freq_dict2 = corpus_frequency(corpus_list2, ignore=ignore_list, calc='freq', normed=True)\n",
    "freq_dict3 = corpus_frequency(corpus_list3, ignore=ignore_list, calc='freq', normed=True)\n",
    "\n",
    "datasets = {\n",
    "    \"dataset1\": corpus_list1,\n",
    "    \"dataset2\": corpus_list2,\n",
    "}\n",
    "\n",
    "def button1_clicked():\n",
    "    create_count_window()\n",
    "\n",
    "def button2_clicked():\n",
    "    create_list_window()\n",
    "\n",
    "def button3_clicked():\n",
    "    create_pos_window()\n",
    "# ボタンを作成\n",
    "button1 = tk.Button(window, text=\"count\", command=button1_clicked)\n",
    "button2 = tk.Button(window, text=\"list\", command=button2_clicked)\n",
    "button3 = tk.Button(window, text=\"pos\", command=button3_clicked)\n",
    "\n",
    "# ウィジェットを配置\n",
    "button1.pack(pady=10)\n",
    "button2.pack(pady=10)\n",
    "button3.pack(pady=10)\n",
    "\n",
    "# イベントループを開始\n",
    "window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
